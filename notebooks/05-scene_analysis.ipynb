{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Guia do Projeto - Tech Challenge Fase 4 - Parte 05\n",
                "\n",
                "## Análise de Cenas e Geração de Resumo\n",
                "\n",
                "Até agora, processamos o vídeo frame a frame. Porém, para um humano, um vídeo é feito de **histórias** ou **cenas**, não de frames isolados. Uma cena é um segmento contínuo que acontece no mesmo lugar e tempo.\n",
                "\n",
                "Neste notebook, vamos:\n",
                "1.  **Detectar Cenas Automaticamente**: Usar histogramas de cor para identificar quando a câmera muda de ângulo ou local (cortes).\n",
                "2.  **Agregar Dados**: Em vez de dizer \"Frame 1: Feliz, Frame 2: Feliz...\", diremos \"Cena 1 (0s-5s): Emoção Dominante = Feliz\".\n",
                "3.  **Gerar Relatório**: Criar uma tabela resumo com contagem de pessoas, emoções e ações por cena.\n",
                "\n",
                "### Como funciona a Detecção de Cenas?\n",
                "\n",
                "Usamos uma técnica simples e eficaz baseada em **Histogramas de Cor**.\n",
                "*   Calculamos a distribuição de cores (histograma) do frame atual.\n",
                "*   Comparamos com o histograma do frame anterior.\n",
                "*   Se a diferença for muito grande (maior que um limiar), assumimos que houve um **corte** de cena."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "import sys\n",
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from dataclasses import dataclass, field\n",
                "from typing import List, Optional, Tuple, Dict, Deque, Any, Generator\n",
                "from collections import deque, Counter\n",
                "from ultralytics import YOLO\n",
                "from deepface import DeepFace\n",
                "import logging\n",
                "\n",
                "os.environ[\"YOLO_VERBOSE\"] = \"False\"\n",
                "logging.basicConfig(level=logging.INFO)\n",
                "logger = logging.getLogger(__name__)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Classes de Suporte (Consolidadas)\n",
                "\n",
                "Para este notebook funcionar sozinho, incluímos todas as classes criadas nas partes anteriores, além das novas classes de Cena."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Estruturas de Dados ---\n",
                "\n",
                "@dataclass\n",
                "class BoundingBox:\n",
                "    x: int; y: int; width: int; height: int\n",
                "\n",
                "@dataclass\n",
                "class FaceDetection:\n",
                "    bounding_box: BoundingBox; confidence: float\n",
                "\n",
                "@dataclass\n",
                "class EmotionAnalysis:\n",
                "    emotion: str; confidence: float\n",
                "\n",
                "@dataclass\n",
                "class ActivityDetection:\n",
                "    activity: str; confidence: float; track_id: Optional[int] = None\n",
                "\n",
                "@dataclass\n",
                "class Scene:\n",
                "    scene_id: int\n",
                "    start_frame: int\n",
                "    end_frame: int\n",
                "    start_time: float\n",
                "    end_time: float\n",
                "    @property\n",
                "    def duration_seconds(self) -> float: return self.end_time - self.start_time\n",
                "\n",
                "@dataclass\n",
                "class SceneResult:\n",
                "    scene: Scene\n",
                "    unique_faces: int = 0\n",
                "    dominant_emotions: Dict[str, int] = field(default_factory=dict)\n",
                "    dominant_actions: Dict[str, int] = field(default_factory=dict)\n",
                "\n",
                "# --- Detectores (Versões Simplificadas para Demonstração) ---\n",
                "# Em produção, usaríamos as versões completas dos notebooks anteriores\n",
                "\n",
                "class FaceDetector:\n",
                "    def __init__(self, model_path: str = \"yolo11n-pose.pt\"):\n",
                "        self.model = YOLO(model_path)\n",
                "    \n",
                "    def detect(self, frame) -> List[FaceDetection]:\n",
                "        # Detecção simplificada\n",
                "        results = self.model(frame, verbose=False)\n",
                "        faces = []\n",
                "        if results[0].boxes:\n",
                "            for box in results[0].boxes:\n",
                "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
                "                faces.append(FaceDetection(BoundingBox(x1, y1, x2-x1, y2-y1), float(box.conf)))\n",
                "        return faces\n",
                "\n",
                "class EmotionAnalyzer:\n",
                "    def analyze(self, frame, faces) -> List[EmotionAnalysis]:\n",
                "        # Simulação para demonstração rápida (DeepFace é lento sem GPU)\n",
                "        # Em produção, descomentar o código real do notebook 03\n",
                "        return [EmotionAnalysis(\"neutral\", 0.9) for _ in faces]\n",
                "\n",
                "class ActivityDetector:\n",
                "    def __init__(self, model_path: str = \"yolo11n-pose.pt\"):\n",
                "        self.model = YOLO(model_path)\n",
                "        \n",
                "    def detect(self, frame) -> List[ActivityDetection]:\n",
                "        # Detecção simplificada com tracking\n",
                "        results = self.model.track(frame, persist=True, verbose=False, tracker=\"bytetrack.yaml\")\n",
                "        activities = []\n",
                "        if results[0].boxes and results[0].boxes.id is not None:\n",
                "            ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
                "            for i, track_id in enumerate(ids):\n",
                "                # Lógica simplificada: assumindo \"standing\" para teste\n",
                "                activities.append(ActivityDetection(\"standing\", 0.9, track_id))\n",
                "        return activities\n",
                "\n",
                "# --- Detector de Cenas (O Novo Protagonista) ---\n",
                "\n",
                "class SceneDetector:\n",
                "    def __init__(self, threshold=0.5):\n",
                "        self.threshold = threshold\n",
                "    \n",
                "    def detect_scenes(self, video_path: str) -> List[Scene]:\n",
                "        cap = cv2.VideoCapture(video_path)\n",
                "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
                "        scenes = []\n",
                "        scene_start = 0\n",
                "        scene_id = 1\n",
                "        \n",
                "        ret, prev = cap.read()\n",
                "        if not ret: return []\n",
                "        \n",
                "        # Calcular histograma do primeiro frame\n",
                "        prev_hist = cv2.calcHist([prev], [0], None, [256], [0, 256])\n",
                "        cv2.normalize(prev_hist, prev_hist, 0, 1, cv2.NORM_MINMAX)\n",
                "        \n",
                "        frame_num = 1\n",
                "        while True:\n",
                "            ret, curr = cap.read()\n",
                "            if not ret: break\n",
                "            \n",
                "            # Calcular histograma do frame atual\n",
                "            curr_hist = cv2.calcHist([curr], [0], None, [256], [0, 256])\n",
                "            cv2.normalize(curr_hist, curr_hist, 0, 1, cv2.NORM_MINMAX)\n",
                "            \n",
                "            # Comparar histogramas (Correlação)\n",
                "            # 1.0 = idêntico, < 0.5 = muito diferente (corte)\n",
                "            score = cv2.compareHist(prev_hist, curr_hist, cv2.HISTCMP_CORREL)\n",
                "            \n",
                "            if score < self.threshold:\n",
                "                # Corte detectado! Salvar cena anterior\n",
                "                scenes.append(Scene(scene_id, scene_start, frame_num-1, scene_start/fps, (frame_num-1)/fps))\n",
                "                scene_id += 1\n",
                "                scene_start = frame_num\n",
                "            \n",
                "            prev_hist = curr_hist\n",
                "            frame_num += 1\n",
                "            \n",
                "        # Salvar a última cena\n",
                "        scenes.append(Scene(scene_id, scene_start, frame_num-1, scene_start/fps, (frame_num-1)/fps))\n",
                "        cap.release()\n",
                "        return scenes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. O Analisador de Vídeo (Orquestrador)\n",
                "\n",
                "Esta classe gerencia tudo: detecta as cenas, e depois percorre cada cena extraindo informações (amostrando frames para não ficar lento)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "class VideoSceneAnalyzer:\n",
                "    def __init__(self, video_path, scene_threshold=0.6, sample_rate=5):\n",
                "        self.video_path = video_path\n",
                "        self.sample_rate = sample_rate # Processar 1 a cada N frames\n",
                "        self.scene_detector = SceneDetector(scene_threshold)\n",
                "        self.face_detector = FaceDetector()\n",
                "        self.emotion_analyzer = EmotionAnalyzer()\n",
                "        self.activity_detector = ActivityDetector()\n",
                "        \n",
                "    def analyze(self) -> List[SceneResult]:\n",
                "        print(\"1. Detectando cenas (pode demorar um pouco)...\")\n",
                "        scenes = self.scene_detector.detect_scenes(self.video_path)\n",
                "        print(f\"   -> {len(scenes)} cenas detectadas.\")\n",
                "        \n",
                "        results = []\n",
                "        cap = cv2.VideoCapture(self.video_path)\n",
                "        \n",
                "        for scene in scenes:\n",
                "            print(f\"2. Processando Cena {scene.scene_id} ({scene.duration_seconds:.1f}s)...\")\n",
                "            cap.set(cv2.CAP_PROP_POS_FRAMES, scene.start_frame)\n",
                "            \n",
                "            emotions_cnt = Counter()\n",
                "            actions_cnt = Counter()\n",
                "            unique_faces = set()\n",
                "            \n",
                "            curr = scene.start_frame\n",
                "            while curr <= scene.end_frame:\n",
                "                ret, frame = cap.read()\n",
                "                if not ret: break\n",
                "                \n",
                "                # Amostragem para performance\n",
                "                if (curr - scene.start_frame) % self.sample_rate == 0:\n",
                "                    # Detectar Atividades e IDs únicos\n",
                "                    acts = self.activity_detector.detect(frame)\n",
                "                    for a in acts:\n",
                "                        actions_cnt[a.activity] += 1\n",
                "                        if a.track_id: unique_faces.add(a.track_id)\n",
                "                    \n",
                "                    # Detectar Emoções\n",
                "                    faces = self.face_detector.detect(frame)\n",
                "                    ems = self.emotion_analyzer.analyze(frame, faces)\n",
                "                    for e in ems: emotions_cnt[e.emotion] += 1\n",
                "                \n",
                "                curr += 1\n",
                "            \n",
                "            results.append(SceneResult(\n",
                "                scene, \n",
                "                len(unique_faces), \n",
                "                dict(emotions_cnt.most_common(2)), # Top 2 emoções\n",
                "                dict(actions_cnt.most_common(2))   # Top 2 ações\n",
                "            ))\n",
                "            \n",
                "        cap.release()\n",
                "        return results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Execução e Relatório\n",
                "\n",
                "Vamos rodar o analisador e formatar a saída em uma tabela legível."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1. Detectando cenas (pode demorar um pouco)...\n",
                        "   -> 18 cenas detectadas.\n",
                        "2. Processando Cena 1 (6.0s)...\n",
                        "2. Processando Cena 2 (6.0s)...\n",
                        "2. Processando Cena 3 (6.0s)...\n",
                        "2. Processando Cena 4 (6.0s)...\n",
                        "2. Processando Cena 5 (6.0s)...\n",
                        "2. Processando Cena 6 (6.0s)...\n",
                        "2. Processando Cena 7 (6.0s)...\n",
                        "2. Processando Cena 8 (6.0s)...\n",
                        "2. Processando Cena 9 (6.0s)...\n",
                        "2. Processando Cena 10 (3.0s)...\n",
                        "2. Processando Cena 11 (4.0s)...\n",
                        "2. Processando Cena 12 (6.0s)...\n",
                        "2. Processando Cena 13 (12.0s)...\n",
                        "2. Processando Cena 14 (1.0s)...\n",
                        "2. Processando Cena 15 (12.0s)...\n",
                        "2. Processando Cena 16 (6.0s)...\n",
                        "2. Processando Cena 17 (12.0s)...\n",
                        "2. Processando Cena 18 (0.8s)...\n",
                        "\n",
                        "=== Relatório de Análise de Cenas ===\n",
                        " Cena Início (s) Duração (s)  Pessoas Únicas Emoções Dominantes Ações Dominantes\n",
                        "    1        0.0         6.0               4        neutral(74)     standing(72)\n",
                        "    2        6.0         6.0               1        neutral(18)     standing(18)\n",
                        "    3       12.0         6.0               0                                    \n",
                        "    4       18.0         6.0               1        neutral(20)     standing(18)\n",
                        "    5       24.0         6.0               1        neutral(18)     standing(18)\n",
                        "    6       30.0         6.0               1        neutral(18)     standing(18)\n",
                        "    7       36.0         6.0               1        neutral(18)     standing(18)\n",
                        "    8       42.0         6.0               1        neutral(18)     standing(18)\n",
                        "    9       48.0         6.0               1        neutral(18)     standing(18)\n",
                        "   10       54.0         3.0               1         neutral(9)      standing(9)\n",
                        "   11       57.0         4.0               1        neutral(12)     standing(12)\n",
                        "   12       61.0         6.0               2        neutral(33)     standing(32)\n",
                        "   13       67.0        12.0               3        neutral(81)     standing(67)\n",
                        "   14       79.0         1.0               1         neutral(3)      standing(3)\n",
                        "   15       80.0        12.0               6       neutral(128)    standing(123)\n",
                        "   16       92.0         6.0               1        neutral(14)     standing(12)\n",
                        "   17       98.0        12.0               9       neutral(148)    standing(131)\n",
                        "   18      110.0         0.8               1         neutral(3)      standing(3)\n"
                    ]
                }
            ],
            "source": [
                "video_path = \"meu_video.mp4\"\n",
                "\n",
                "try:\n",
                "    # Threshold 0.6 geralmente funciona bem para cortes secos\n",
                "    analyzer = VideoSceneAnalyzer(video_path, scene_threshold=0.6, sample_rate=10)\n",
                "    scene_results = analyzer.analyze()\n",
                "\n",
                "    # Gerar DataFrame para visualização\n",
                "    data = []\n",
                "    for res in scene_results:\n",
                "        scene = res.scene\n",
                "        emotions_str = \", \".join([f\"{k}({v})\" for k, v in res.dominant_emotions.items()])\n",
                "        actions_str = \", \".join([f\"{k}({v})\" for k, v in res.dominant_actions.items()])\n",
                "        \n",
                "        data.append({\n",
                "            \"Cena\": scene.scene_id,\n",
                "            \"Início (s)\": f\"{scene.start_time:.1f}\",\n",
                "            \"Duração (s)\": f\"{scene.duration_seconds:.1f}\",\n",
                "            \"Pessoas Únicas\": res.unique_faces,\n",
                "            \"Emoções Dominantes\": emotions_str,\n",
                "            \"Ações Dominantes\": actions_str\n",
                "        })\n",
                "\n",
                "    df = pd.DataFrame(data)\n",
                "    print(\"\\n=== Relatório de Análise de Cenas ===\")\n",
                "    print(df.to_string(index=False))\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"Erro: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Conclusão da Parte 05\n",
                "\n",
                "Neste notebook, elevamos o nível da análise de frames individuais para **cenas completas**.\n",
                "\n",
                "**O que foi construído:**\n",
                "*   **Detector de Cenas:** Implementação robusta usando histogramas de cor para identificar cortes.\n",
                "*   **Agregador de Dados:** Lógica para consolidar emoções e atividades dominantes por cena.\n",
                "*   **Relatório Estruturado:** Geração de um DataFrame com o resumo narrativo do vídeo.\n",
                "\n",
                "**Próximo Passo:**\n",
                "No notebook final (**06-final_pipeline.ipynb**), vamos integrar todos os componentes (Vídeo, Face, Emoção, Atividade, Cena) em uma única classe `VideoPipeline` para processar qualquer vídeo de ponta a ponta."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (Tech Challenge Fase 04)",
            "language": "python",
            "name": "fiap-tech-challenge-fase04"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
